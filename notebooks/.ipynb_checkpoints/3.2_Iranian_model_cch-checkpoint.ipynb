{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 15px; height: 80px\">\n",
    "\n",
    "# TROLL HUNTING: DETECTING STATE-BACKED DISINFORMATION CAMPAIGNS ON TWITTER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTEOOK 3.2: ALTERNATIVE IRAN MODEL AND EVALUATION\n",
    "The first model we built with a mixture of Russian and American tweets did well in picking out unseen Russian fake tweets. But it failed to do as well with unseen Iranian and Venezuelian fake tweets which contained words that the model was not familiar with, or saw very little of in its training set.\n",
    "\n",
    "So this raises an obvious question: Would a similar model trained with Iranian fake tweets do as well in detecting unseen Iranian tweets? Let's find out by repeating the steps in creating our first model, but using 25,000 Iranian state-backed tweets this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, f1_score, mean_squared_error, precision_score, recall_score\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "\n",
    "mpl.rcParams[\"figure.dpi\"] = 300\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format ='retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. PRE-PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling up a new Iranian set created for this test\n",
    "iran_en = pd.read_csv('../data/iran_full_en.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting this new Iran dataset into 2 - 1 for training, 1 as an unseen test set\n",
    "iran_train = iran_en.sample(n=25000, random_state=42)\n",
    "iran_unseen = iran_en.sample(n=1000, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeating the same cleaning steps\n",
    "def clean_tweet(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub(\"\\W\", \" \", text)\n",
    "    text = re.sub(\"\\s+\", \" \", text)\n",
    "    text = text.strip(\" \")\n",
    "    text = text.strip(\"\\n\")\n",
    "    text = re.sub(\"[^\\w\\s]\", \"\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "iran_train['clean_tweet_text'] = iran_train['tweet_text'].map(lambda tweet: clean_tweet(tweet))\n",
    "iran_unseen['clean_tweet_text'] = iran_unseen['tweet_text'].map(lambda tweet: clean_tweet(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "iran_train['clean_tweet_text'] = iran_train['clean_tweet_text'].dropna().copy()\n",
    "iran_unseen['clean_tweet_text'] = iran_unseen['clean_tweet_text'].dropna().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling up the same set of real tweets used to train the first model \n",
    "real = pd.read_csv('../data/real.csv')\n",
    "real_train = real.sample(n=25000, random_state=7).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a new col to classify the real Vs state-backed tweets\n",
    "iran_train['bot_or_not'] = 1\n",
    "iran_unseen['bot_or_not'] = 1\n",
    "real_train['bot_or_not'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "iran5050 = pd.concat((iran_train, real_train), axis=0, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "iran5050['clean_tweet_text'] = iran5050['clean_tweet_text'].fillna(\"missing text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = iran5050['bot_or_not']\n",
    "X = iran5050['clean_tweet_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    6256\n",
       "1    6244\n",
       "Name: bot_or_not, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. SETTING A BASELINE PERFORMANCE VIA DUMMY CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = DummyClassifier(strategy='stratified', random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DummyClassifier(constant=None, random_state=42, strategy='stratified')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_pred = dummy.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.50864\n",
      "Precision Score :  0.5080645161290323\n",
      "Recall Score: 0.5145739910313901\n",
      "f1 Score: 0.5112985359643539\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy Score:\", accuracy_score(y_test, dummy_pred))\n",
    "print(\"Precision Score : \", precision_score(y_test, dummy_pred))\n",
    "print(\"Recall Score:\", recall_score(y_test, dummy_pred))\n",
    "print(\"f1 Score:\", f1_score(y_test, dummy_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. NEW LOGREG PIPELINE\n",
    "Since we've established the LogReg model as the best one in notebook 3.0, I shall stick with that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"LogReg_Iran\": (\n",
    "        Pipeline(\n",
    "            [\n",
    "                (\"vect\", CountVectorizer()),\n",
    "                (\"tfidf\", TfidfTransformer()),\n",
    "                (\"clf\", LogisticRegression()),\n",
    "            ]\n",
    "        ),\n",
    "        {\n",
    "            \"vect__stop_words\": [\"english\"],\n",
    "            \"vect__min_df\": [1],\n",
    "            \"vect__max_df\": (0.5, 1.0),\n",
    "            \"vect__max_features\": [None],\n",
    "            \"vect__ngram_range\": [(1, 2)],  \n",
    "            \"tfidf__use_idf\": [True, False],\n",
    "            \"tfidf__norm\": [\"l2\"],\n",
    "            \"clf__penalty\": [\"l2\"],\n",
    "            \"clf__C\": np.logspace(-4, 4, 40),\n",
    "            \"clf__solver\": [\"liblinear\"],\n",
    "        },\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- LogReg_Iran -----\n"
     ]
    }
   ],
   "source": [
    "for name, (model, parameters) in models.items():\n",
    "    print(\"----- {} -----\".format(name))\n",
    "    gs = GridSearchCV(model, parameters, cv=5, verbose=0, n_jobs=-1, scoring=\"roc_auc\")\n",
    "    gs.fit(X_train, y_train)\n",
    "    print(\"Best Parameters:\", gs.best_params_)\n",
    "    print(\"\")\n",
    "    print(\"Best Score:\", gs.best_score_)\n",
    "\n",
    "    joblib.dump(gs.best_estimator_, f\"{name}.pkl\", compress=1)\n",
    "    joblib.dump(gs.cv_results_, f\"{name}_results.pkl\", compress=1)\n",
    "\n",
    "    y_pred = gs.predict(X_test)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Precision Score :\", precision_score(y_test, y_pred))\n",
    "    print(\"Recall Score:\", recall_score(y_test, y_pred))\n",
    "    print(\"f1 Score:\", f1_score(y_test, y_pred))\n",
    "    # print(classification_report(y_test, y_pred))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interestingly, the LogReg model for the Iranian training set performed better than the LogReg model for the Russian set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. TESTING THE IRAN LOGREG MODEL AGAINST AN UNSEEN TEST SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_iran = joblib.load(\"LogReg_Iran.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing the finalised LR model the full training data-set, ahead of testing it on unseen data\n",
    "LR_iran.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 A 50-50 TEST\n",
    "Let's start with an easy test, with an even mix of fake and real tweets among the 100 tweets that we will test the model on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recalling the third test-set of real tweets created earlier for this purpose \n",
    "real_test4 = pd.read_csv('../data/real_test4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_test4['clean_tweet_text'] = real_test4['tweet_text'].map(lambda tweet: clean_tweet(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a new col to classify the real Vs state-backed tweets\n",
    "iran_unseen['bot_or_not'] = 1\n",
    "real_test4['bot_or_not'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iran_new_sample = pd.concat((iran_unseen, real_test4), axis=0, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab a sample of 100 tweets from the unseen set\n",
    "iranian_test = iran_new_sample.sample(n=100, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_iranian = iranian_test['bot_or_not']\n",
    "X_iranian = iranian_test['clean_tweet_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_iranian.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_iranian5050 = LR_iran.predict(X_iranian)\n",
    "pred_iranian5050"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_proba_iranian5050 = LR_iran.predict_proba(X_iranian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_iranian5050 = confusion_matrix(y_iranian, pred_iranian5050)\n",
    "cm_iranian5050"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "sns.heatmap(cm_iranian5050, annot=True, fmt=\"d\", cmap=\"gist_gray_r\")\n",
    "sns.set(font_scale=1.8)\n",
    "plt.title(\"Predictions v Actual \")\n",
    "plt.ylabel(\"Actual \")\n",
    "plt.xlabel(\"Predicted \")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy Score:\", accuracy_score(y_iranian, pred_iranian5050))\n",
    "print(\"Precision Score :\", precision_score(y_iranian, pred_iranian5050))\n",
    "print(\"Recall Score:\", recall_score(y_iranian, pred_iranian5050))\n",
    "print(\"f1 Score:\", f1_score(y_iranian, pred_iranian5050))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANALYSIS:\n",
    "The Iran LogReg model did amazingly well on its first test with an unseen test set, wrongly classifying just 5 out of 100 tweets. Of the 47 fake tweets, the model correctly classified 44 of them. It wrongly classified 2 real tweets as fake tweets, and classified 3 fake tweets as real tweets.\n",
    "\n",
    "This was an even better performance than the \"Russian-trained\" LogReg model.\n",
    "\n",
    "Let's give the model a tougher challenge by reducing the number of fake tweets in the test sample and see if it can still classify correctly. I'll start with a 70-30 split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 A 70-30 TEST\n",
    "This time, I'll shift the sampling weight such that the unseen test set has about 70% real tweets and 30% fake tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iran_new_sample['70real_30fake'] = np.where(iran_new_sample['bot_or_not'] == 0, 0.7, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iranian_test7030 = iran_new_sample.sample(n=100, random_state=42, weights='70real_30fake')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_iranian7030 = iranian_test7030['bot_or_not']\n",
    "X_iranian7030 = iranian_test7030['clean_tweet_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The split is not precise, but good enough\n",
    "y_iranian7030.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_unseen7030 = LR_iran.predict(X_iranian7030)\n",
    "pred_unseen7030"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_unseen7030 = confusion_matrix(y_iranian7030, pred_unseen7030)\n",
    "cm_unseen7030"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "sns.heatmap(cm_unseen7030, annot=True, fmt=\"d\", cmap=\"gist_gray_r\")\n",
    "sns.set(font_scale=1.8)\n",
    "plt.title(\"Predictions v Actual \")\n",
    "plt.ylabel(\"Actual \")\n",
    "plt.xlabel(\"Predicted \")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy Score:\", accuracy_score(y_iranian7030, pred_unseen7030))\n",
    "print(\"Precision Score :\", precision_score(y_iranian7030, pred_unseen7030))\n",
    "print(\"Recall Score:\", recall_score(y_iranian7030, pred_unseen7030))\n",
    "print(\"f1 Score:\", f1_score(y_iranian7030, pred_unseen7030))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANALYSIS:\n",
    "The Iranian model turns in another strong strong performance despite the reduction in number of fake tweets, correctly classifying 94 tweets out of 100. It caught 33 of the 35 fake tweets, and only incorrectly classified 4 tweets as fake, when they are in fact real.\n",
    "\n",
    "Let's see how the model performs in a 90-10 split, with even fewer fake tweets this time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 A 90-10 TEST\n",
    "Now I'll shift the sampling weight such that the unseen test set has about 90% real tweets and 10% fake tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iran_new_sample['90real_10fake'] = np.where(iran_new_sample['bot_or_not'] == 0, 0.9, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iranian_test9010 = iran_new_sample.sample(n=100, random_state=42, weights='90real_10fake')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_iranian9010 = iranian_test9010['bot_or_not']\n",
    "X_iranian9010 = iranian_test9010['clean_tweet_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Again, not precise but enough for the purpose of testing\n",
    "y_iranian9010.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_unseen9010 = LR_iran.predict(X_iranian9010)\n",
    "pred_unseen9010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_unseen9010 = confusion_matrix(y_iranian9010, pred_unseen9010)\n",
    "cm_unseen9010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "sns.heatmap(cm_unseen9010, annot=True, fmt=\"d\", cmap=\"gist_gray_r\")\n",
    "sns.set(font_scale=1.8)\n",
    "plt.title(\"Predictions v Actual \")\n",
    "plt.ylabel(\"Actual \")\n",
    "plt.xlabel(\"Predicted \")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy Score:\", accuracy_score(y_iranian9010, pred_unseen9010))\n",
    "print(\"Precision Score :\", precision_score(y_iranian9010, pred_unseen9010))\n",
    "print(\"Recall Score:\", recall_score(y_iranian9010, pred_unseen9010))\n",
    "print(\"f1 Score:\", f1_score(y_iranian9010, pred_unseen9010))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANALYSIS:\n",
    "Surprisingly, the LogReg model continues to be able to pick out the fake tweets with ease, this time correctly classifying 14 out of all 14 fake tweets in the test set.\n",
    "\n",
    "The model made just 1 mistake in this particular split - wrongly classifying 1 tweet as fake when it is in fact real.\n",
    "\n",
    "The results from the 3 different sets of test show clearly that there's no \"global\" or generic solution when it comes to attempts to flush out these state operators on Twitter.\n",
    "\n",
    "The most effective models are targetted and specific, trained on a set of tweets which have already betrayed the operator's particular style and focus. \n",
    "\n",
    "Put another way, a model trained on fake tweets from one state-operator isn't going to do well in catching the tweets by another state operator. This seems obvious, but can yet be a false assumption many could take in this topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
